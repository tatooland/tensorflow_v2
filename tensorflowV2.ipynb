{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflowV2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "4G2cnRxt7CjP",
        "Y0QdA6DFedpf",
        "o0JSFtCVa9bQ",
        "vMTlilRcbLPS"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatooland/tensorflow_v2/blob/master/tensorflowV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOwbq4X_MWc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "print(tf.__version__)\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G2cnRxt7CjP",
        "colab_type": "text"
      },
      "source": [
        "##创建简单网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3R_j3vv7Jk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.Input(shape=(784,), name='img')\n",
        "h1 = layers.Dense(32, activation='relu')(inputs)\n",
        "h2 = layers.Dense(32, activation='relu')(h1)\n",
        "outputs = layers.Dense(10, activation='softmax')(h2)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "tf.keras.utils.plot_model(model, 'mnist_model.png')\n",
        "tf.keras.utils.plot_model(model, 'model_info.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGx9zYzrAlH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=400, epochs=20, validation_split=.2)\n",
        "test_scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('test loss:', test_scores[0])\n",
        "print('test acc:', test_scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0QdA6DFedpf",
        "colab_type": "text"
      },
      "source": [
        "#使用共享网络创建多个模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnqJSr2Jo2Bh",
        "colab_type": "text"
      },
      "source": [
        "#DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20_E1uN0ayrm",
        "colab_type": "text"
      },
      "source": [
        "##define google driver file downloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcaMSkA_a4kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def downloadFromGDriver(fileId):\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  fileName = fileId + '.zip'\n",
        "  downloaded = drive.CreateFile({'id': fileId})\n",
        "  downloaded.GetContentFile(fileName)\n",
        "  ds = ZipFile(fileName)\n",
        "  ds.extractall()\n",
        "  os.remove(fileName)\n",
        "  print('Extracted zip file ' + fileName)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ckcBE9Vnmwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloadFromGDriver(\"1dCEGqATdkr5uMi22TI1IYFN7NjK9V4oh\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbfOSP7Ln-Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo55-9xyoc4s",
        "colab_type": "text"
      },
      "source": [
        "##读取数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5x_MUVrojdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_img_path(parentNodePath):\n",
        "  return listdir(parentNodePath)\n",
        "\n",
        "def parse_image(fileName):\n",
        "  print(fileName)\n",
        "  image = tf.io.read_file(fileName)\n",
        "  image = tf.image.decode_png(image, channels=3)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = tf.image.resize(image, [128, 128])\n",
        "  \n",
        "  return image\n",
        "\n",
        "def show(image):\n",
        "  print(image.shape)\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "\n",
        "images_path = [\"/content/images/%s\" % img_path for img_path in get_img_path(\"/content/images/\") if isfile(\"/content/images/%s\" % img_path)]\n",
        "image_ds = tf.data.Dataset.from_tensor_slices((np.array(images_path)))\n",
        "image_ds = image_ds.map(parse_image)\n",
        "\n",
        "print(int(len(images_path)))\n",
        "\n",
        "train_ds = image_ds.batch(int(len(images_path) * .3)).repeat()\n",
        "test_ds = image_ds.batch(int(len(images_path) * .3)).repeat()\n",
        "\n",
        "BATCH_SIZE = int(len(images_path) * .3)\n",
        "print(BATCH_SIZE)\n",
        "#show(parse_image('/content/images/wooper.png'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYeJ-k78k1XM",
        "colab_type": "text"
      },
      "source": [
        "##generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yaw_9Jf8Lk2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator():\n",
        "  #target_shape = (H, W, C) = (128, 128, 3)\n",
        "  \n",
        "  input_val = tf.keras.Input(shape=(100,), name='noise')\n",
        "  input = layers.Dense(4 * 4 * 1024, input_shape=(100,), use_bias=False)(input_val)\n",
        "  input = layers.LeakyReLU()(input)\n",
        "\n",
        "  g = layers.Reshape((4, 4, 1024))(input)\n",
        "\n",
        "  g = layers.Conv2DTranspose(512, 3, strides=(2, 2), use_bias=False, padding='same')(g)\n",
        "  g = layers.BatchNormalization()(g)\n",
        "  g = layers.LeakyReLU()(g)\n",
        "  #assert g.shape() == (None, 8, 8, 512)\n",
        "\n",
        "  g = layers.Conv2DTranspose(256, 3, strides=(2, 2), use_bias=False, padding='same')(g)\n",
        "  g = layers.BatchNormalization()(g)\n",
        "  g = layers.LeakyReLU()(g)\n",
        "  #assert g.shape() == (None, 16, 16, 256)\n",
        "\n",
        "  g = layers.Conv2DTranspose(128, 3, strides=(2, 2), use_bias=False, padding='same')(g)\n",
        "  g = layers.BatchNormalization()(g)\n",
        "  g = layers.LeakyReLU()(g)\n",
        "  #assert g.shape() == (None, 32, 32, 128)\n",
        "\n",
        "  g = layers.Conv2DTranspose(64, 3, strides=(2, 2), use_bias=False, padding='same')(g)\n",
        "  g = layers.BatchNormalization()(g)\n",
        "  g = layers.LeakyReLU()(g)\n",
        "  #assert g.shape() == (None, 64, 64, 64)\n",
        "\n",
        "  output = layers.Conv2DTranspose(3, 3, strides=(2, 2), use_bias=False, activation='tanh', padding='same')(g)\n",
        "  #assert g.shape() == (None, 128, 128, 3)\n",
        "\n",
        "  return tf.keras.Model(inputs=input_val, outputs=output)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHe_GqhBm1w9",
        "colab_type": "text"
      },
      "source": [
        "###生成图片"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fyZ6eRIm1CB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "noise = tf.random.normal([1, 100])\n",
        "g = generator()\n",
        "g_img = g(noise, training=False)\n",
        "print(g_img.shape)\n",
        "plt.imshow(g_img[0, :, :, 0], cmap='gray' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RvHsS31JwrZ",
        "colab_type": "text"
      },
      "source": [
        "##DISCRIMINATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndmAusAhJXQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator():\n",
        "  input_val = tf.keras.Input(shape=(128, 128, 3))\n",
        "  input = layers.Conv2D(64, 3, strides=(2, 2), padding='same')(input_val)\n",
        "  input = layers.LeakyReLU()(input)\n",
        "  input = layers.Dropout(.3)(input)\n",
        "  #(64, 64, 64)\n",
        "\n",
        "  input = layers.Conv2D(128, 3, strides=(2, 2), padding='same')(input)\n",
        "  input = layers.LeakyReLU()(input)\n",
        "  input = layers.Dropout(.3)(input)\n",
        "  #(32, 32, 128)\n",
        "\n",
        "  input = layers.Conv2D(256, 3, strides=(2, 2), padding='same')(input)\n",
        "  input = layers.LeakyReLU()(input)\n",
        "  input = layers.Dropout(.3)(input)\n",
        "  #(16, 16, 256)\n",
        "\n",
        "  input = layers.Conv2D(512, 3, strides=(2, 2), padding='same')(input)\n",
        "  input = layers.LeakyReLU()(input)\n",
        "  input = layers.Dropout(.3)(input)\n",
        "  #(8, 8, 512)\n",
        "\n",
        "  input = layers.Conv2D(1024, 3, strides=(2, 2), padding='same')(input)\n",
        "  input = layers.LeakyReLU()(input)\n",
        "  input = layers.Dropout(.3)(input)\n",
        "  #(4, 4, 1024)\n",
        "\n",
        "  input = layers.Flatten()(input)\n",
        "  input = layers.Dense(4*4*1024)(input)\n",
        "  output = layers.Dense(1)(input)\n",
        "\n",
        "  return tf.keras.Model(inputs=input_val, outputs=output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-vGByOoYFPU",
        "colab_type": "text"
      },
      "source": [
        "##DISCRIMINATOR LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7UOkfavYKPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combined_loss(d_real_output, d_fake_output):\n",
        "  bce = tf.keras.losses.BinaryCrossentropy()\n",
        "  dr_loss = bce(tf.ones_like(d_real_output), d_real_output)\n",
        "  df_loss = bce(tf.zeros_like(d_fake_output), d_fake_output)\n",
        "  return dr_loss + df_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpBmkxTh9CX3",
        "colab_type": "text"
      },
      "source": [
        "##GENERATOR LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptNvzRL9HjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(g_fake_output):\n",
        "  bce = tf.keras.losses.BinaryCrossentropy();\n",
        "  return bce(tf.ones_like(g_fake_output), g_fake_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqmMRvaQAUfq",
        "colab_type": "text"
      },
      "source": [
        "##OPTIMIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q_CNbYvAS-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimizer():\n",
        "  g_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "  d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "  return g_optimizer, d_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYDLZMgNEQS_",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN STEP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcDw8jHCEO6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@tf.function\n",
        "def train_step(images, noise_dim, BATCH_SIZE):\n",
        "  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "  with tf.GradientTape() as g_grad_tape, tf.GradientTape() as d_grad_tape:\n",
        "    g = generator()\n",
        "    d = discriminator()\n",
        "    g_fake_images = g(noise, training=True)\n",
        "    \n",
        "    d_fake_output = d(g_fake_images, training=True)\n",
        "    d_real_output = d(images, training=True)\n",
        "\n",
        "    d_loss = combined_loss(d_real_output, d_fake_output)\n",
        "    g_loss = generator_loss(d_fake_output)\n",
        "\n",
        "  dg = g_grad_tape.gradient(g_loss, g.trainable_variables)\n",
        "  dd = d_grad_tape.gradient(d_loss, d.trainable_variables)\n",
        "\n",
        "  g_opt, d_opt = optimizer()\n",
        "\n",
        "  g_opt.apply_gradients(zip(dg, g.trainable_variables))\n",
        "  d_opt.apply_gradients(zip(dd, d.trainable_variables))\n",
        "  return g_fake_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43gMQ9bZNvuP",
        "colab_type": "text"
      },
      "source": [
        "##EPOCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNkUA2btNuD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "epoch = 0\n",
        "\n",
        "#g_opt, d_opt = optimizer()\n",
        "#g = generator()\n",
        "#d = discriminator()\n",
        "\n",
        "#checkpoint_dir = './training_checkpoints'\n",
        "#checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "#checkpoint = tf.train.Checkpoint(generator_optimizer=g_opt, discriminator_optimizer=d_opt, generator=g, discriminator=d)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  for img_batch in train_ds:\n",
        "    imgs = train_step(img_batch, noise_dim, BATCH_SIZE)\n",
        "    show(imgs[BATCH_SIZE - 1])\n",
        "  #if epoch % 15 == 0:\n",
        "    #checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0JSFtCVa9bQ",
        "colab_type": "text"
      },
      "source": [
        "##define keras image geneerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgceq6slbEFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "def readImgDataAsFlow(train_dir, test_dir, validation_dir, target_size=(128, 128)):\n",
        "  trainGenerator = ImageDataGenerator(data_format=\"channels_last\")\n",
        "  testGenerator = ImageDataGenerator(data_format=\"channels_last\")\n",
        "  validationGenerator = ImageDataGenerator(data_format=\"channels_last\")\n",
        "  trainFlow = trainGenerator.flow_from_directory(train_dir, class_mode='categorical', target_size=target_size)\n",
        "  testFlow = testGenerator.flow_from_directory(test_dir, class_mode='categorical', target_size=target_size)\n",
        "  validationFlow = validationGenerator.flow_from_directory(validation_dir, class_mode='categorical', target_size=target_size)\n",
        "  return trainFlow, testFlow, validationFlow\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMTlilRcbLPS",
        "colab_type": "text"
      },
      "source": [
        "##保存模型和序列化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbAEdFuia9Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model_save.h5')\n",
        "del model\n",
        "model = tf.keras.models.load_model('model_save.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGfcG__Pehu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode_input = tf.keras.Input(shape=(28, 28, 1), name='img')\n",
        "h1 = layers.Conv2D(16, 3, activation='relu')(encode_input)\n",
        "h1 = layers.Conv2D(32, 3, activation='relu')(h1)\n",
        "h1 = layers.MaxPool2D(3)(h1)\n",
        "h1 = layers.Conv2D(32, 3, activation='relu')(h1)\n",
        "h1 = layers.Conv2D(16, 3, activation='relu')(h1)\n",
        "encode_output = layers.GlobalMaxPool2D(h1)\n",
        "\n",
        "encode_model = tf.keras.Model(inputs=encode_input, outputs=encode_output)\n",
        "encode_model.summary()\n",
        "\n",
        "h2 = layers.Reshape((4, 4, 1))(encode_output)\n",
        "h2 = layers.Conv2DTranspose(16, 3, activation='relu')(h2)\n",
        "h2 = layers.Conv2DTranspose(32, 3, activation='relu')(h2)\n",
        "\n",
        "h2 = layers.UpSampling2D(3)(h2)\n",
        "h2 = layers.Conv2DTranspose(16, 3, activation='relu')(h2)\n",
        "decode_output = layers.Conv2DTranspose(1, 3, activation='relu')(h2)\n",
        "\n",
        "autoencoder = keras.Model(inputs=encode_input, outputs=decode_output)\n",
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}